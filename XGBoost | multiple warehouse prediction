{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":80874,"databundleVersionId":8794587,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ondrejmajor/rohlik-orders-prediction-xgboost-w-result-plots?scriptVersionId=191438494\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Rohlík Data Preprocessing\nThis notebook contains a time series forecasting task on Rohlik dataset. EDA, data preprocessing and forecasting engine is XGBoost Model. \n\n<br>This notebook and the EDA of this notebook comes from my previous work with deep learning LSTM approach [Rohlik Orders Prediction | LSTM](https://www.kaggle.com/code/ondrejmajor/rohlik-orders-prediction-lstm-w-result-plots)\n\n<br><b>Approach:</b>\n<br>As Rohlík orders dataset contains data from multiple warehouses, thus it contains multiple timelines, the approach is to process and predict each warehouse as a separate timeline. The preprocessing, fitting and evaluation steps are the same for each warehouse. Off days are inserted in timelines that does not contain sundays and some other missing days in the dataset.\n\n<br>This approach yields very good MAPE value, which is calculated as a mean of all warehouse timeline results.\n\n<b>This notebook is written with end-to-end mindset as if the models are to be deployed. Models, scalers and training configurations are saved. The test dataset is predicted on with inference style. The predictions can be done offline without the trainig notebook.","metadata":{}},{"cell_type":"code","source":"import os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy import interpolate\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, RobustScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import ElasticNet, LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import (\n    RandomForestRegressor,\n    HistGradientBoostingRegressor,\n    GradientBoostingRegressor,\n    BaggingRegressor,\n    ExtraTreesRegressor,\n    AdaBoostRegressor,\n    VotingRegressor\n)\nfrom sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\n\n\nOFF_DAYS = True # Experimental. Fill missing days in timeseries as \"offday\" and fill features and target with 0s. \nLAG = False # add lag and rolling features\nFILL = True # fill nan with zeros before training\nSCALER = \"minmax\" # standard\n\n#WAREHOUSE = [\"Budapest_1\", \"Munich_1\"] # Train only particular warehouse timelines (For testing purposes)\nWAREHOUSE = [None]  # Train on all warehouses (whole dataset)\n\n\n# Temporal features\nLAG_DAYS = [63, 168, 364]  # this lag bridges the whole test dataset with past data\nROLLING_WINDOW = [63, 168, 364]  # this lag bridges the whole test dataset with past data\n\npd.set_option('display.max_columns', None)  # see all columns in pandas\n\ntrain_df_raw = pd.read_csv(\"/kaggle/input/rohlik-orders-forecasting-challenge/train.csv\")\ntrain_calendar_df_raw = pd.read_csv(\"/kaggle/input/rohlik-orders-forecasting-challenge/train_calendar.csv\")\ntest_df_raw = pd.read_csv(\"/kaggle/input/rohlik-orders-forecasting-challenge/test.csv\")\ntest_calendar_df_raw = pd.read_csv(\"/kaggle/input/rohlik-orders-forecasting-challenge/test_calendar.csv\")\n\ntrain_df_raw['date'] = pd.to_datetime(train_df_raw['date'])\ntrain_calendar_df_raw['date'] = pd.to_datetime(train_calendar_df_raw['date'])\ntest_df_raw['date'] = pd.to_datetime(test_df_raw['date'])\ntest_calendar_df_raw['date'] = pd.to_datetime(test_calendar_df_raw['date'])","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:26.784397Z","iopub.execute_input":"2024-08-06T17:16:26.784869Z","iopub.status.idle":"2024-08-06T17:16:45.51783Z","shell.execute_reply.started":"2024-08-06T17:16:26.784833Z","shell.execute_reply":"2024-08-06T17:16:45.516601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Optional - Dataset Validation\n\nEvaluate the correctness of the data from multiple sources. Check both .csvs if their values in the overlapping columns match and then merge them into one dataset.\n<br>After that we get train_df and test_df dataframes.","metadata":{}},{"cell_type":"code","source":"def merge_csv(train_df, train_calendar_df):\n    # Identify common columns\n    common_columns = set(train_df.columns).intersection(set(train_calendar_df.columns))\n    print(common_columns)\n\n    # Merg the two datasets on warehouse and date\n    merged_df_to_test = pd.merge(train_df, train_calendar_df, on=['date', 'warehouse'], suffixes=('_train', '_calendar'))\n    \n        # Compare values in common columns\n    differences = {}\n    for column in common_columns:\n        if column not in ['date', 'warehouse']:  # Exclude join keys from comparison\n            train_col = f\"{column}_train\"\n            calendar_col = f\"{column}_calendar\"\n            \n            # Check if there are any differences, excluding rows where both values are NaN\n            diff = merged_df_to_test[(merged_df_to_test[train_col] != merged_df_to_test[calendar_col]) & \n                            (~merged_df_to_test[train_col].isna() | ~merged_df_to_test[calendar_col].isna())]\n            if not diff.empty:\n                differences[column] = diff[['date', 'warehouse', train_col, calendar_col]]\n            else:\n                print(f\"No differences found in column: {column}\")\n\n    # Display only the rows where values were different\n    for col, diff in differences.items():\n        print(f\"\\nNot merged. Differences found in column: {col}\")\n        print(diff)\n        return\n\n    if not differences:\n        print(\"\\nAll values match across the datasets, datasets merged.\")\n        return pd.merge(train_df, train_calendar_df, on=list(common_columns))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:45.520013Z","iopub.execute_input":"2024-08-06T17:16:45.520388Z","iopub.status.idle":"2024-08-06T17:16:45.530084Z","shell.execute_reply.started":"2024-08-06T17:16:45.520358Z","shell.execute_reply":"2024-08-06T17:16:45.528816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merg the two datasets on 'date' and 'warehouse' to facilitate comparison\ntrain_df_merged = merge_csv(train_df_raw, train_calendar_df_raw)\ntest_df_merged = merge_csv(test_df_raw, test_calendar_df_raw) ","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:45.531932Z","iopub.execute_input":"2024-08-06T17:16:45.532436Z","iopub.status.idle":"2024-08-06T17:16:45.632847Z","shell.execute_reply.started":"2024-08-06T17:16:45.532397Z","shell.execute_reply":"2024-08-06T17:16:45.631685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort the datasets by 'date' and 'warehouse' ascendingly\n\ntrain_df = train_df_merged.sort_values(by=['date', 'warehouse'])\ntest_df = test_df_merged.sort_values(by=['date', 'warehouse'])","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:45.6342Z","iopub.execute_input":"2024-08-06T17:16:45.634539Z","iopub.status.idle":"2024-08-06T17:16:45.650811Z","shell.execute_reply.started":"2024-08-06T17:16:45.634512Z","shell.execute_reply":"2024-08-06T17:16:45.649513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Drop unnecesary data\nThere are some features in the training set that are not availiable in the testing set, so we do not use them.\n<br>The \"orders\" column we keep as it is our target.","metadata":{}},{"cell_type":"code","source":"# get features that are not available in the test dataset\nunavailable_features = list(set(train_df.columns).difference(set(test_df.columns)))    \nunavailable_features.remove('orders')\n\nprint(f\"Common features: {unavailable_features}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:45.653565Z","iopub.execute_input":"2024-08-06T17:16:45.654584Z","iopub.status.idle":"2024-08-06T17:16:45.660437Z","shell.execute_reply.started":"2024-08-06T17:16:45.654549Z","shell.execute_reply":"2024-08-06T17:16:45.659264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.drop(columns=unavailable_features)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:45.66197Z","iopub.execute_input":"2024-08-06T17:16:45.662279Z","iopub.status.idle":"2024-08-06T17:16:45.672544Z","shell.execute_reply.started":"2024-08-06T17:16:45.662248Z","shell.execute_reply":"2024-08-06T17:16:45.671423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Columns that have unchanging values or very little movement, sometimes redundant features.\n<br> We look as binary features that might be alway 0 or 1.\n<br> \"school_holidays\" and \"shops_closed\" are redundant in some warehouses\n<br> We can drop these columns as they are redundant but in this run we leave it.","metadata":{}},{"cell_type":"code","source":"# Run the code to identify and drop unchanging columns\n\nfor warehouse in train_df['warehouse'].unique():\n    unchanging_columns = []\n    print(f\"\\nWarehouse: {warehouse}\")\n    for feature in train_df.columns:\n        if feature in ['warehouse', 'date']:\n            continue\n\n        values = train_df[train_df['warehouse'] == warehouse][feature].nunique()\n        if values == 1:\n            print(f\"Feature {feature} has {values} unique values\")\n            print(f\"Value: {train_df[train_df['warehouse'] == warehouse][feature].unique()}\")\n            if feature not in unchanging_columns:\n                unchanging_columns.append(feature)\n        if values == 2:\n            print(f\"Feature {feature} has {values} unique values\")\n            true_count = sum(train_df[train_df['warehouse'] == warehouse][feature] == 1)\n            total_count = len(train_df[train_df['warehouse'] == warehouse])\n            print(f\"{true_count} items True of {total_count}\")\n\n    print(f\"    Unchanging columns in {warehouse}: {unchanging_columns}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:45.67421Z","iopub.execute_input":"2024-08-06T17:16:45.674948Z","iopub.status.idle":"2024-08-06T17:16:45.850384Z","shell.execute_reply.started":"2024-08-06T17:16:45.674917Z","shell.execute_reply":"2024-08-06T17:16:45.849057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What is the gap between the date of the last training data and the first test data?\ntrain_max_date = train_df['date'].max()\ntest_min_date = test_df['date'].min()\nprint(f\"Train max date: {train_max_date}\")\nprint(f\"Test min date: {test_min_date}\")\nprint(f\"Gap: {test_min_date - train_max_date}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:45.852262Z","iopub.execute_input":"2024-08-06T17:16:45.852749Z","iopub.status.idle":"2024-08-06T17:16:45.86482Z","shell.execute_reply.started":"2024-08-06T17:16:45.85269Z","shell.execute_reply":"2024-08-06T17:16:45.860488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Experimental - Line-up all warehouse timelines in the same time format.\nThis is experimental function with a purpose:\n\n-   When we examine the dataset, we can see that some warehouses do not operate at sundays. \n-   Some warehouses are missing a lot of days rom first months as they were just setting up oin new markets.\n-   There are missing days on other warehouses sometimes.\n-   <b>This function introduces</b> a new feature <b>\"off_day\"</b>, which is 1 whenever we have missing day in the dataset.\n- Currently diabled, uncomment last line bellow to experiment.","metadata":{}},{"cell_type":"code","source":"def process_warehouses(train_df, handle_off_days='interpolate'):\n    \"\"\"\n    Process each warehouse and handle off days based on the specified method.\n    \n    Parameters:\n    train_df (DataFrame): The input DataFrame containing warehouse data.\n    handle_off_days (str): The method to handle off days, either 'interpolate' or 'zeros'.\n    \n    Returns:\n    DataFrame: The processed DataFrame with all warehouses combined.\n    \"\"\"\n    combined_df_list = []\n    start_dates_info = []\n\n    for warehouse in train_df['warehouse'].unique():\n        wh = train_df[train_df['warehouse'] == warehouse]\n        \n        # Fix specific warehouses with unconisitent data at the start\n        if warehouse == 'Frankfurt_1':\n            first_month_end = wh['date'].min() + pd.DateOffset(days=4)\n            wh = wh[wh['date'] > first_month_end]\n        if warehouse == 'Munich_1':\n            first_month_end = wh['date'].min() + pd.DateOffset(weeks=5)\n            wh = wh[wh['date'] > first_month_end]\n            \n        wh.set_index('date', inplace=True)\n        date_range = pd.date_range(start=wh.index.min(), end=wh.index.max(), freq='D')\n        wh_reindexed = wh.reindex(date_range).reset_index()\n        wh_reindexed.rename(columns={'index': 'date'}, inplace=True)\n        \n        # Add off_day column\n        wh_reindexed['off_day'] = wh_reindexed['orders'].isna().astype(int)\n        \n        # Here we have to fill in the missing features for the off days, mostly 0s here, we do not count holidays, etc. for missing days for now.\n        wh_reindexed['warehouse'] = warehouse\n        wh_reindexed['holiday_name'] = wh_reindexed['holiday_name'].fillna('')\n        wh_reindexed['holiday'] = wh_reindexed['holiday'].fillna(0)\n        wh_reindexed['shops_closed'] = wh_reindexed['shops_closed'].fillna(0)\n        wh_reindexed['winter_school_holidays'] = wh_reindexed['winter_school_holidays'].fillna(0)\n        wh_reindexed['school_holidays'] = wh_reindexed['school_holidays'].fillna(0)\n        \n        if handle_off_days == 'interpolate':\n            wh_reindexed['orders'] = wh_reindexed['orders'].interpolate(method='linear')\n        elif handle_off_days == 'zeros':\n            wh_reindexed['orders'] = wh_reindexed['orders'].fillna(0.0)\n        \n        # the 'id' column is for the test dataset mostly\n        wh_reindexed['id'] = wh_reindexed.apply(lambda row: f\"{row['warehouse']}_{row['date'].strftime('%Y-%m-%d')}\", axis=1)\n        wh_reindexed['day_of_week'] = wh_reindexed['date'].dt.dayofweek + 1  # Monday=1, Sunday=7\n        \n        # Cut off data to start on the earliest Monday - all warehouses start on timeline on Monday, so they hawe weekly overlap.\n        start_date = wh_reindexed['date'].min()\n        end_date = wh_reindexed['date'].max()\n        start_monday = start_date - pd.Timedelta(days=start_date.weekday())\n        end_sunday = end_date + pd.Timedelta(days=(6 - end_date.weekday()))\n        wh_reindexed = wh_reindexed[(wh_reindexed['date'] >= start_monday) & (wh_reindexed['date'] <= end_sunday)]\n        \n        # get info about the start date \n        start_dates_info.append(f\"{warehouse}: Start Date: {start_monday.strftime('%Y-%m-%d')} (Day {start_monday.weekday() + 1})\")\n\n        # get all new dataframes into a list\n        combined_df_list.append(wh_reindexed)\n        \n        # Plot\n        plt.figure(figsize=(20, 10))\n        plt.plot(wh_reindexed['date'], wh_reindexed['orders'], marker='x', linestyle='-', markersize=3, linewidth=0.5, label='Orders')\n\n        # start of the first week and end of the last week\n        plt.axvline(x=start_monday, color='r', linestyle='--', linewidth=0.5, label='Start of first week')\n        plt.axvline(x=end_sunday, color='g', linestyle='--', linewidth=0.5, label='End of last week')\n        \n        # Highlight the off days\n        off_days = wh_reindexed[wh_reindexed['off_day'] == 1]\n        plt.scatter(off_days['date'], off_days['orders'], color='red', s=15, label='Off days', marker='o')\n        \n        # Mark the weekdays on off days\n        for idx, row in wh_reindexed.iterrows():\n            if row['off_day'] == 1:\n                plt.text(row['date'], row['orders'], str(row['day_of_week']), color='black', ha='center', va='bottom', fontsize=10)\n\n        plt.title(f\"Volume for warehouse {warehouse}\")\n        plt.xlabel('Date')\n        plt.ylabel('Orders')\n        plt.legend()\n        plt.xticks(rotation=45)\n        plt.show()\n\n        # Drop day_of_week column , we will be adding it later as a feature\n        wh_reindexed.drop(columns=['day_of_week'], inplace=True)\n\n    # Print start dates for each warehouse\n    print(\"Start Dates for Each Warehouse:\")\n    for info in start_dates_info:\n        print(info)\n\n    # Combine all warehouses into one DataFrame\n    train_df_reindexed = pd.concat(combined_df_list, ignore_index=True)\n    \n    return train_df_reindexed\n\n# Uncomment to use this function\nif OFF_DAYS:\n    train_df = process_warehouses(train_df, handle_off_days='zeros') # zero or interpolate - zero makes more sense but still confuses the model durin training. Interpolate is purely experimental.","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:45.867254Z","iopub.execute_input":"2024-08-06T17:16:45.867661Z","iopub.status.idle":"2024-08-06T17:16:52.362062Z","shell.execute_reply.started":"2024-08-06T17:16:45.867632Z","shell.execute_reply":"2024-08-06T17:16:52.360792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are merging train and test datasets and process it as one dataset, so we can apply temporal features on both datasets. We will get lag days on the test dataset. Later we split dataset back to train and test for training and prediction.","metadata":{}},{"cell_type":"code","source":"# merge the two datasets\ntrain_df = pd.concat([train_df, test_df], axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:52.363494Z","iopub.execute_input":"2024-08-06T17:16:52.363844Z","iopub.status.idle":"2024-08-06T17:16:52.373731Z","shell.execute_reply.started":"2024-08-06T17:16:52.363817Z","shell.execute_reply":"2024-08-06T17:16:52.372294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In our test dataset is not a lot of holidays that had inpact on \"orders\" in previous years so we are not going to use it. \n<br>It could be used with one-hot encoding and a bit experimentuing.","metadata":{}},{"cell_type":"code","source":"#train_df = train_df.drop(columns=['holiday_name'])\ntrain_df['holiday_name'] = train_df['holiday_name'].fillna('no_hol')","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:52.37553Z","iopub.execute_input":"2024-08-06T17:16:52.376012Z","iopub.status.idle":"2024-08-06T17:16:52.40124Z","shell.execute_reply.started":"2024-08-06T17:16:52.37597Z","shell.execute_reply":"2024-08-06T17:16:52.399971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.sort_values(by=['date', 'warehouse'], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:52.402633Z","iopub.execute_input":"2024-08-06T17:16:52.402998Z","iopub.status.idle":"2024-08-06T17:16:52.417829Z","shell.execute_reply.started":"2024-08-06T17:16:52.40297Z","shell.execute_reply":"2024-08-06T17:16:52.416659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(7)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:52.419184Z","iopub.execute_input":"2024-08-06T17:16:52.419512Z","iopub.status.idle":"2024-08-06T17:16:52.445412Z","shell.execute_reply.started":"2024-08-06T17:16:52.419486Z","shell.execute_reply":"2024-08-06T17:16:52.444244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# only get data of one warehouse for the dataset - for testing purposes\nif WAREHOUSE != [None]:\n    train_df = train_df[train_df['warehouse'].isin(WAREHOUSE)]\n    print(\"ran\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:52.451379Z","iopub.execute_input":"2024-08-06T17:16:52.451788Z","iopub.status.idle":"2024-08-06T17:16:52.457254Z","shell.execute_reply.started":"2024-08-06T17:16:52.451753Z","shell.execute_reply":"2024-08-06T17:16:52.455981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Produce date features\nThis function processes the date column and produces multiple time features from it.","metadata":{}},{"cell_type":"code","source":"from math import pi\ndef process_date(df):\n    \n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Extract date-related features\n    df['quarter'] = df['date'].dt.quarter\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['day_of_week'] = df['date'].dt.dayofweek\n    df['day_of_year'] = df['date'].dt.dayofyear\n    df['week_of_year'] = df['date'].dt.isocalendar().week.astype(int)\n    \n    \n    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n    \n    df['day_before_holiday'] = df['holiday'].shift().fillna(0)\n    df['day_after_holiday'] = df['holiday'].shift(-1).fillna(0)\n    df['day_before_school_holiday'] = df['school_holidays'].shift().fillna(0)\n    df['day_after_school_holiday'] = df['school_holidays'].shift(-1).fillna(0)\n    df['holiday_and_shops_closed'] = df['holiday'] * df['shops_closed']\n    \n    df[\"is_month_start\"] = df['date'].dt.is_month_start.astype(int).fillna(0)\n    df[\"is_month_end\"] = df['date'].dt.is_month_end.astype(int).fillna(0)\n    df[\"is_quarter_start\"] = df['date'].dt.is_quarter_start.astype(int).fillna(0)\n    df[\"is_quarter_end\"] = df['date'].dt.is_quarter_end.astype(int).fillna(0)\n    \n    # Calculate days since the start date\n    df['days_since_start'] = (df['date'] - df['date'].min()).dt.days\n    \n    \n    # Create cyclic features\n    def cyclical_encode(df, col, max_val):\n        df[f'{col}_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n        df[f'{col}_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n\n    cyclical_encode(df, 'month', 12)\n    cyclical_encode(df, 'day_of_week', 7)\n    cyclical_encode(df, 'day', 31)\n    cyclical_encode(df, 'quarter', 4)\n    \n    \n    cat_features = ['day_of_week', 'quarter', 'month', 'day', 'year', 'week_of_year', 'holiday_name']\n    bin_features = ['is_weekend', 'is_quarter_end', 'is_quarter_start', 'is_month_end', 'is_month_start', 'day_before_holiday', 'day_after_holiday', 'day_before_school_holiday', 'day_after_school_holiday', 'holiday_and_shops_closed']\n    \n    return df, cat_features, bin_features","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:52.459083Z","iopub.execute_input":"2024-08-06T17:16:52.459433Z","iopub.status.idle":"2024-08-06T17:16:52.475526Z","shell.execute_reply.started":"2024-08-06T17:16:52.459406Z","shell.execute_reply":"2024-08-06T17:16:52.474429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply process_date function and save the categorical and binary features for later use.\n\ntrain_df, categorical_features, binary_features=process_date(train_df)\n\nif OFF_DAYS:\n    binary_features = binary_features + ['holiday', 'shops_closed', 'winter_school_holidays', 'school_holidays', 'off_day']\nelse:\n    binary_features = binary_features + ['holiday', 'shops_closed', 'winter_school_holidays', 'school_holidays']","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:52.476922Z","iopub.execute_input":"2024-08-06T17:16:52.477274Z","iopub.status.idle":"2024-08-06T17:16:52.529174Z","shell.execute_reply.started":"2024-08-06T17:16:52.477246Z","shell.execute_reply":"2024-08-06T17:16:52.528099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We keep track of important parameters and settings during the training phase, as it is good habbit when later deploying machine learning models for inference.\n<br>Here we are tracking features that will be passed through the scalers.","metadata":{}},{"cell_type":"code","source":"x_scaler_features = list(set(train_df.columns) - set(categorical_features) - set(binary_features) - {'warehouse', 'orders', 'id', 'date'})\nprint(x_scaler_features)\nprint(categorical_features)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:52.530455Z","iopub.execute_input":"2024-08-06T17:16:52.530797Z","iopub.status.idle":"2024-08-06T17:16:52.537465Z","shell.execute_reply.started":"2024-08-06T17:16:52.530767Z","shell.execute_reply":"2024-08-06T17:16:52.536297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if \"off_day\" in binary_features:\n    train_df['off_day'] = train_df['off_day'].fillna(0)\n\n    orders = train_df[\"orders\"]\n    train_df = train_df.fillna(0)\n    train_df[\"orders\"] = orders","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:52.53895Z","iopub.execute_input":"2024-08-06T17:16:52.539389Z","iopub.status.idle":"2024-08-06T17:16:52.553804Z","shell.execute_reply.started":"2024-08-06T17:16:52.539351Z","shell.execute_reply":"2024-08-06T17:16:52.552642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review dataset before proecssing\ntrain_df.head(7)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:52.555309Z","iopub.execute_input":"2024-08-06T17:16:52.556256Z","iopub.status.idle":"2024-08-06T17:16:52.595961Z","shell.execute_reply.started":"2024-08-06T17:16:52.556214Z","shell.execute_reply":"2024-08-06T17:16:52.594651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation Matrix","metadata":{}},{"cell_type":"code","source":"train_df_corr = train_df.drop(columns=['warehouse', \"id\", \"holiday_name\"]).corr()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:52.597877Z","iopub.execute_input":"2024-08-06T17:16:52.598364Z","iopub.status.idle":"2024-08-06T17:16:52.636554Z","shell.execute_reply.started":"2024-08-06T17:16:52.598332Z","shell.execute_reply":"2024-08-06T17:16:52.635344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.heatmap(train_df_corr, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title('Correlation Matrix')\nplt.show()\n\n# Print the correlation with 'orders'\nprint(train_df_corr['orders'].sort_values(ascending=False))","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:52.637931Z","iopub.execute_input":"2024-08-06T17:16:52.63839Z","iopub.status.idle":"2024-08-06T17:16:55.449034Z","shell.execute_reply.started":"2024-08-06T17:16:52.638353Z","shell.execute_reply":"2024-08-06T17:16:55.447897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Produce temporal features\nCreate lag and rolling window features.","metadata":{}},{"cell_type":"code","source":"def create_lag_features(df, col, lag_days):\n    df = df.copy()\n    lag_features = []\n    for lag in lag_days:\n        feature_name = f'{col}_lag_{lag}'\n        df[feature_name] = df[col].shift(lag)\n        lag_features.append(feature_name)\n    return df, lag_features\n\ndef create_rolling_features(df, col, windows):\n    df = df.copy()\n    window_features = []\n    for window in windows:\n        mean_feature = f'{col}_rolling_mean_{window}'\n        std_feature = f'{col}_rolling_std_{window}'\n        \n        df[mean_feature] = df[col].rolling(window, min_periods=1).mean()\n        df[std_feature] = df[col].rolling(window, min_periods=1).std()\n        \n        window_features.extend([mean_feature, std_feature])\n    return df, window_features","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:55.450904Z","iopub.execute_input":"2024-08-06T17:16:55.451305Z","iopub.status.idle":"2024-08-06T17:16:55.462427Z","shell.execute_reply.started":"2024-08-06T17:16:55.451273Z","shell.execute_reply":"2024-08-06T17:16:55.460976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(df, cat_features, x_scaler_features, binary_features, redundant_features=[], inference=False, fill_method='ffill', lag = True):\n    warehouse = df['warehouse'].iloc[0]\n    redundant_df = df[redundant_features]\n    df = df.drop(columns=redundant_features)\n    \n    if inference is False and lag is True:\n        # Create lag and rolling features\n        df, lag_features = create_lag_features(df, 'orders', LAG_DAYS)\n        df, window_features = create_rolling_features(df, 'orders', ROLLING_WINDOW)\n        \n        x_scaler_features = x_scaler_features + lag_features + window_features\n        \n        # Handle NaN values in lag and rolling features\n        if fill_method == 'ffill':\n            df[lag_features + window_features] = df[lag_features + window_features].ffill().bfill()\n            orders = df[\"orders\"]\n            df = df.ffill().bfill()\n            df[\"orders\"] = orders\n            \n        elif fill_method == 'zero':\n            df[lag_features + window_features] = df[lag_features + window_features].fillna(0)\n            orders = df[\"orders\"]\n            df = df.fillna(0)\n            df[\"orders\"] = orders\n    \n    # One-hot encoding for categorical features\n    if not inference:\n        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n        encoded_features = encoder.fit_transform(df[cat_features])\n        joblib.dump(encoder, f'onehot_scaler_{warehouse}.joblib')\n    else:\n        encoder = joblib.load(f'onehot_scaler_{warehouse}.joblib')\n        encoded_features = encoder.transform(df[cat_features])\n\n    encoded_feature_names = encoder.get_feature_names_out(cat_features)\n    encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names, index=df.index)\n\n    # Prepare features for scaling\n    if inference is False:\n        X = df.drop(columns=['orders'])\n        y = df['orders']\n    else:\n        X = df\n\n    # Initialize and fit/transform scalers\n    if not inference:\n        if SCALER == \"minmax\":\n            x_scaler = MinMaxScaler(feature_range=(0, 1))\n            y_scaler = MinMaxScaler(feature_range=(0, 1))\n        elif SCALER == \"robust\":\n            x_scaler = RobustScaler()\n            y_scaler = RobustScaler()\n        else:\n            x_scaler = StandardScaler()\n            y_scaler = StandardScaler()\n        X_scaled = x_scaler.fit_transform(X[x_scaler_features])\n        y_scaled = y_scaler.fit_transform(y.values.reshape(-1, 1))\n        joblib.dump(x_scaler, f'x_scaler_{warehouse}.joblib')\n        joblib.dump(y_scaler, f'y_scaler_{warehouse}.joblib')\n    else:\n        x_scaler = joblib.load(f'x_scaler_{warehouse}.joblib')\n        X_scaled = x_scaler.transform(X[x_scaler_features])\n        #y_scaler = joblib.load(f'y_scaler_{warehouse}.joblib')\n        #y_scaled = y_scaler.transform(y.values.reshape(-1, 1))\n\n    # Create DataFrame with scaled features\n    X_scaled_df = pd.DataFrame(X_scaled, columns=x_scaler_features, index=df.index)\n\n    # Combine all features\n    final_df = pd.concat([\n        X_scaled_df,  # Scaled numerical features\n        encoded_df,   # One-hot encoded categorical features\n        df[binary_features],  # Binary features (unchanged)\n        redundant_df  # Add redundant features back for the test data export\n    ], axis=1)\n    \n    if inference:\n        print(final_df.shape)\n        return final_df\n    else:\n        print(final_df.shape, y_scaled.shape)\n        return final_df, y_scaled","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:55.464061Z","iopub.execute_input":"2024-08-06T17:16:55.464431Z","iopub.status.idle":"2024-08-06T17:16:55.485892Z","shell.execute_reply.started":"2024-08-06T17:16:55.464402Z","shell.execute_reply":"2024-08-06T17:16:55.484799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing Function\nThis is preprocessing function that applies:\n<br>lag features\n<br>one-hot encoding of categorical features\n<br>feature-target split\n<br>feature, target scaling\n<br>return the final dataframe","metadata":{}},{"cell_type":"markdown","source":"## Outliers Reduction Function\nFunction to fix outliers - orders that are very high or very low. \n<br>Plot the outliers.\n<br>Settings of the z-score and window that is outliers calculated with.","metadata":{}},{"cell_type":"code","source":"if not OFF_DAYS:\n    def z_score_outlier_detection(data, window=14, threshold=3):\n        data = data.sort_index()\n        rolling_mean = data.rolling(window=window, center=True, min_periods=1).mean()\n        rolling_std = data.rolling(window=window, center=True, min_periods=1).std()\n        z_scores = (data - rolling_mean) / rolling_std\n        outliers = (np.abs(z_scores) > threshold) & (data != 0)\n        lower_bound = rolling_mean - (threshold * rolling_std)\n        upper_bound = rolling_mean + (threshold * rolling_std)\n        return outliers, lower_bound, upper_bound\n\n\n    def weighted_trend_interpolation(data, outliers, window_size=10, weight=0.5):\n        fixed_data = data.copy()\n        for idx in np.where(outliers)[0]:\n            start_idx = max(0, idx - window_size)\n            end_idx = min(len(data), idx + window_size + 1)\n\n            prev_values = data.iloc[start_idx:idx].dropna()\n            next_values = data.iloc[idx + 1:end_idx].dropna()\n\n            #print(f\"Previous values: {prev_values}\\n\")\n            #print(f\"Next values: {next_values}\\n\")\n\n            if len(prev_values) == 0:\n                correction = next_values.mean()\n            elif len(next_values) == 0:\n                correction = prev_values.mean()\n            else:\n                mean_prev = prev_values.mean()\n                mean_next = next_values.mean()\n\n                if mean_next > mean_prev:\n                    trend = 'up'\n                else:\n                    trend = 'down'\n\n                # Calculate new value based on trend and retain some of the original intensity\n                if trend == 'up':\n                    correction = mean_prev + (next_values.mean() - mean_prev) / 2\n                else:\n                    correction = mean_prev - (mean_prev - next_values.mean()) / 2\n\n            if np.isnan(correction):  # Handle cases where both prev_values and next_values are NaN\n                correction = data.iloc[idx]\n\n            fixed_data.iloc[idx] = weight * data.iloc[idx] + (1 - weight) * correction\n\n            #print(f\"Outlier at index {idx}: {data.iloc[idx]} -> {fixed_data.iloc[idx]}\")\n        return fixed_data\n\n\n    def plot_outlier_comparison(df, window=28, z_threshold=3, window_size=10, weight=0.5):\n        warehouse_data = df['orders']\n\n        z_outliers, z_lower, z_upper = z_score_outlier_detection(warehouse_data, window, z_threshold)\n        fixed_warehouse_data = weighted_trend_interpolation(warehouse_data, z_outliers, window_size, weight)\n\n        # Plot the original data and outliers\n        plt.figure(figsize=(20, 10))\n        plt.plot(warehouse_data.index, warehouse_data, label='Orders', alpha=0.7)\n        plt.fill_between(warehouse_data.index, z_lower, z_upper, color='green', alpha=0.1, label='Z-Score Bounds')\n        plt.scatter(warehouse_data.index[z_outliers], warehouse_data[z_outliers], color='blue', label='Z-Score Outliers', marker='s')\n\n        # Plot fixed outliers\n        plt.scatter(warehouse_data.index[z_outliers], fixed_warehouse_data[z_outliers], color='green', label='Fixed Outliers', marker='x')\n        plt.title(f'{df[\"warehouse\"].iloc[0]} - Outlier Comparison')\n        plt.xlabel('Date')\n        plt.ylabel('Orders')\n        plt.legend()\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.show()\n\n        print(f\"\\n{df['warehouse'].iloc[0]}:\")\n        print(f\"Z-Score Outliers: {sum(z_outliers)}\")\n\n        # Update the DataFrame with fixed values\n        df.loc[:, 'orders'] = fixed_warehouse_data\n\n        return df\n\n\n\nelse:\n    def z_score_outlier_detection(data, window=14, threshold=3):\n        # Exclude rows where 'off_day' == 1\n        mask = data['off_day'] != 1\n        data_filtered = data[mask].copy()\n        data_filtered['orders'] = data_filtered['orders'].sort_index()\n\n        rolling_mean = data_filtered['orders'].rolling(window=window, center=True, min_periods=1).mean()\n        rolling_std = data_filtered['orders'].rolling(window=window, center=True, min_periods=1).std()\n        z_scores = (data_filtered['orders'] - rolling_mean) / rolling_std\n        outliers_filtered = (np.abs(z_scores) > threshold) & (data_filtered['orders'] != 0)\n        lower_bound_filtered = rolling_mean - (threshold * rolling_std)\n        upper_bound_filtered = rolling_mean + (threshold * rolling_std)\n\n        # Create full-length arrays with NaN for excluded indices\n        outliers = pd.Series(np.nan, index=data.index)\n        lower_bound = pd.Series(np.nan, index=data.index)\n        upper_bound = pd.Series(np.nan, index=data.index)\n\n        outliers[mask] = outliers_filtered\n        lower_bound[mask] = lower_bound_filtered\n        upper_bound[mask] = upper_bound_filtered\n\n        return outliers, lower_bound, upper_bound\n\n    def weighted_trend_interpolation(data, outliers, window_size=10, weight=0.5):\n        fixed_data = data['orders'].copy()\n        for idx in np.where(outliers)[0]:\n            start_idx = max(0, idx - window_size)\n            end_idx = min(len(data), idx + window_size + 1)\n\n            prev_values = data['orders'].iloc[start_idx:idx].dropna()\n            next_values = data['orders'].iloc[idx + 1:end_idx].dropna()\n\n            if len(prev_values) == 0:\n                correction = next_values.mean()\n            elif len(next_values) == 0:\n                correction = prev_values.mean()\n            else:\n                mean_prev = prev_values.mean()\n                mean_next = next_values.mean()\n\n                if mean_next > mean_prev:\n                    trend = 'up'\n                else:\n                    trend = 'down'\n\n                # Calculate new value based on trend and retain some of the original intensity\n                if trend == 'up':\n                    correction = mean_prev + (next_values.mean() - mean_prev) / 2\n                else:\n                    correction = mean_prev - (mean_prev - next_values.mean()) / 2\n\n            if np.isnan(correction):  # Handle cases where both prev_values and next_values are NaN\n                correction = data['orders'].iloc[idx]\n\n            fixed_data.iloc[idx] = weight * data['orders'].iloc[idx] + (1 - weight) * correction\n\n        return fixed_data\n\n    def plot_outlier_comparison(df, window=28, z_threshold=3, window_size=10, weight=0.5):\n        warehouse_data = df[['orders', 'off_day']]\n        non_off_day_data = warehouse_data[warehouse_data['off_day'] != 1]\n\n        z_outliers, z_lower, z_upper = z_score_outlier_detection(warehouse_data, window, z_threshold)\n        fixed_warehouse_data = weighted_trend_interpolation(warehouse_data, z_outliers, window_size, weight)\n\n        # Plot the original data and outliers\n        plt.figure(figsize=(20, 10))\n        plt.plot(non_off_day_data.index, non_off_day_data['orders'], label='Orders', alpha=0.7)\n        plt.fill_between(warehouse_data.index, z_lower, z_upper, color='green', alpha=0.1, label='Z-Score Bounds')\n        plt.scatter(warehouse_data.index[z_outliers.dropna().index], warehouse_data['orders'][z_outliers.dropna().index], color='blue', label='Z-Score Outliers', marker='s')\n\n        # Plot fixed outliers\n        plt.scatter(warehouse_data.index[z_outliers.dropna().index], fixed_warehouse_data[z_outliers.dropna().index], color='green', label='Fixed Outliers', marker='x')\n        plt.title(f'{df[\"warehouse\"].iloc[0]} - Outlier Comparison')\n        plt.xlabel('Date')\n        plt.ylabel('Orders')\n        plt.legend()\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.show()\n\n        print(f\"\\n{df['warehouse'].iloc[0]}:\")\n        print(f\"Z-Score Outliers: {sum(z_outliers.dropna())}\")\n\n        # Update the DataFrame with fixed values\n        df.loc[:, 'orders'] = fixed_warehouse_data\n\n        return df","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:55.487849Z","iopub.execute_input":"2024-08-06T17:16:55.488318Z","iopub.status.idle":"2024-08-06T17:16:55.524244Z","shell.execute_reply.started":"2024-08-06T17:16:55.488279Z","shell.execute_reply":"2024-08-06T17:16:55.522557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:55.526184Z","iopub.execute_input":"2024-08-06T17:16:55.526648Z","iopub.status.idle":"2024-08-06T17:16:55.552248Z","shell.execute_reply.started":"2024-08-06T17:16:55.526607Z","shell.execute_reply":"2024-08-06T17:16:55.551029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_df['date'].min())\nprint(test_df['date'].max())","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:55.554026Z","iopub.execute_input":"2024-08-06T17:16:55.554496Z","iopub.status.idle":"2024-08-06T17:16:55.561942Z","shell.execute_reply.started":"2024-08-06T17:16:55.554456Z","shell.execute_reply":"2024-08-06T17:16:55.560705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess Data for Each Warehouse, Data Visualisations\nPut all above together in a loop that produces:\n\n- Reduces outliers\n- Train dataframe dictionary for every warehouse a dataframe\n- Test dataframe with lag features and scaled \n- Extra dates from train data added to the test dataset to create LSTM sequences from later.\n","metadata":{}},{"cell_type":"code","source":"from datetime import timedelta\n\n\nfeatures = [f'orders_lag_{lag}' for lag in LAG_DAYS]\nfeatures2 = [f'orders_rolling_mean_{window}' for window in ROLLING_WINDOW]\n\nwarehouses = train_df['warehouse'].unique()\n\n# Create dataframes for each warehouse\nwarehouse_dfs = {wh: train_df[train_df['warehouse'] == wh].copy() for wh in warehouses}\n\nprocessed_train_data_dict = {}\nprocessed_test_data = []\n\nprocessed_full_dataset_dict = {}\n\n\nfor warehouse, df in warehouse_dfs.items():\n    print(f\"Processing warehouse: {warehouse}\")\n    \n    \n    \n    df = df.sort_values(by=['date'])\n    \n    # Get the date range for test dataset.\n    test_min_date = test_df['date'].min()\n    test_max_date = test_df['date'].max()\n    \n    # Get the first date of the test data adding extra dates to it for the sequences for LSTM.\n    start_date = test_min_date # no need for time sequences with XGBoost || -(2*timedelta(days=TIME_STEPS))\n    \n    # Get indices for the specified date range for the test dataset.\n    selected_indices = df[(df['date'] >= start_date) & (df['date'] <= test_max_date)].index\n    # Indicies to drop from train data, that only include the real test data from the real test dataset.\n    test_indices = df[(df['date'] >= test_min_date) & (df['date'] <= test_max_date)].index\n    \n    # Plot outlier setup\n    ## Feels like the outliers actually improve predictions on holidais etc.\n    #df = plot_outlier_comparison(df, window=70, z_threshold=3.0, weight=0.5)\n\n    # Preprocess data\n    X_processed, y_processed = preprocess_data(df, \n                                               categorical_features, \n                                               x_scaler_features, \n                                               binary_features, \n                                               redundant_features=['warehouse', 'id', 'date'], \n                                               inference=False, \n                                               fill_method='ffill',\n                                               lag=LAG)\n    \n    \n    processed_full_dataset_dict[warehouse] = (X_processed, y_processed)\n    \n    \n    test_data = X_processed.loc[selected_indices]\n    \n    selected_positions = X_processed.index.get_indexer(selected_indices)\n    test_positions = X_processed.index.get_indexer(test_indices)\n    \n    X_processed = X_processed.drop(test_indices)\n    y_processed = np.delete(y_processed, test_positions)\n    \n    processed_test_data.append(test_data)\n    processed_train_data_dict[warehouse] = (X_processed, y_processed)\n    \n\n    \n    print(f\"Processed:\\ntrain data shape: {X_processed.shape}, {y_processed.shape}\")\n    print(\"test data shape:\", test_data.shape)\n    print(f\"y nans: {sum(np.isnan(y_processed))}\")\n    print(\"\\n\")\n\n\n    \n    \n# create test dataframe for the test data for each warehouse\nprocessed_test_df = pd.concat(processed_test_data)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:55.563682Z","iopub.execute_input":"2024-08-06T17:16:55.564163Z","iopub.status.idle":"2024-08-06T17:16:55.795765Z","shell.execute_reply.started":"2024-08-06T17:16:55.564057Z","shell.execute_reply":"2024-08-06T17:16:55.794328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(len(processed_train_data_dict[\"Prague_1\"][0]))\n#print(len(processed_full_dataset_dict[\"Prague_1\"][0]))","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:55.797419Z","iopub.execute_input":"2024-08-06T17:16:55.797917Z","iopub.status.idle":"2024-08-06T17:16:55.803243Z","shell.execute_reply.started":"2024-08-06T17:16:55.797861Z","shell.execute_reply":"2024-08-06T17:16:55.801749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Export Processed Test Data","metadata":{}},{"cell_type":"code","source":"if FILL:\n    processed_test_df = processed_test_df.fillna(0)\n    processed_test_df.tail()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:55.804647Z","iopub.execute_input":"2024-08-06T17:16:55.80502Z","iopub.status.idle":"2024-08-06T17:16:55.818283Z","shell.execute_reply.started":"2024-08-06T17:16:55.804992Z","shell.execute_reply":"2024-08-06T17:16:55.816978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(processed_test_df.isna().sum())","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:55.819952Z","iopub.execute_input":"2024-08-06T17:16:55.82073Z","iopub.status.idle":"2024-08-06T17:16:55.834157Z","shell.execute_reply.started":"2024-08-06T17:16:55.820662Z","shell.execute_reply":"2024-08-06T17:16:55.832829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Processed test dataset length: {len(processed_test_df)}\")\nprint(f\"Raw test dataset length: {len(test_df_raw)}\")\n# Save the test dataset for future use\nprocessed_test_df.to_csv('/kaggle/working/test_proc_mt.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:55.835609Z","iopub.execute_input":"2024-08-06T17:16:55.836698Z","iopub.status.idle":"2024-08-06T17:16:55.897368Z","shell.execute_reply.started":"2024-08-06T17:16:55.836664Z","shell.execute_reply":"2024-08-06T17:16:55.896125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot the test data\nVisualise the training dataset with new features for every warehouse timeline","metadata":{}},{"cell_type":"code","source":"if LAG:\n    features = [f'orders_lag_{lag}' for lag in LAG_DAYS]\n    features2 = [f'orders_rolling_mean_{window}' for window in ROLLING_WINDOW]\n\n    def is_weekend(date):\n        return date.weekday() >= 5\n\n    # Plot the processed test data:\n\n    for wh in processed_test_df['warehouse'].unique():\n        wh_df = processed_test_df[processed_test_df['warehouse'] == wh]\n        plt.figure(figsize=(20, 10))\n\n        plt.plot(wh_df[\"date\"], wh_df[features[0]], label=features[0])\n        #plt.plot(wh_df[\"date\"], wh_df[features[1]], label=features[1])\n        #plt.plot(wh_df[\"date\"], wh_df[features[2]], label=features[2])\n        #plt.plot(wh_df[\"date\"], wh_df[features[3]], label=features[3])\n        plt.plot(wh_df[\"date\"], wh_df[features2[0]], label=features2[0])\n        plt.plot(wh_df[\"date\"], wh_df[features2[1]], label=features2[1])\n        plt.plot(wh_df[\"date\"], wh_df[features2[2]], label=features2[2])\n\n        # Shade weekends\n        wh_df['is_weekend'] = wh_df['date'].apply(is_weekend)\n        weekends = wh_df[wh_df['is_weekend']]\n        for i, row in weekends.iterrows():\n            plt.axvspan(row['date'], row['date'] + pd.Timedelta(days=1), color='gray', alpha=0.3)\n\n        plt.title(f'{wh} - Processed Test Data')\n        plt.xlabel('Date')\n        plt.ylabel('Orders')\n        plt.legend()\n        plt.xticks(rotation=45)\n        plt.tight_layout()\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:55.898668Z","iopub.execute_input":"2024-08-06T17:16:55.899165Z","iopub.status.idle":"2024-08-06T17:16:55.910208Z","shell.execute_reply.started":"2024-08-06T17:16:55.899078Z","shell.execute_reply":"2024-08-06T17:16:55.908801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot LAG features\nVisualise LAG features of train and test back to back with the real orders","metadata":{}},{"cell_type":"code","source":"if LAG:\n    feature = f'orders_lag_{LAG_DAYS[0]}'  # third lag feature for test data\n\n    # Loop through the warehouse dataframes in the dictionary\n    for warehouse in processed_train_data_dict.keys():\n        plt.figure(figsize=(15, 5))\n\n        # Get train data for the warehouse\n        train_orders = processed_train_data_dict[warehouse][1]  # X_processed\n        train_data = processed_train_data_dict[warehouse][0]  # X_processed\n\n        # Get test data for the warehouse\n        test_data = processed_test_df[processed_test_df['warehouse'] == warehouse]\n\n        # Plot the orders and the feature\n        plt.plot(train_data.index, train_orders, label='Train Orders', color='green')\n        plt.plot(train_data.index, train_data[feature], label=f'Train {feature}', color='blue')\n        plt.plot(test_data.index, test_data[feature], label=f'Test {feature}', color='red')\n        \n        plt.title(f'Train and Test for {warehouse} Warehouse - {feature}')\n        plt.xlabel('Date')\n        plt.ylabel('Orders')\n        plt.legend()\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:55.911851Z","iopub.execute_input":"2024-08-06T17:16:55.912218Z","iopub.status.idle":"2024-08-06T17:16:55.927477Z","shell.execute_reply.started":"2024-08-06T17:16:55.91219Z","shell.execute_reply":"2024-08-06T17:16:55.926269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for warehouse, (X_processed, y_processed) in processed_train_data_dict.items():\n    print(f\"Warehouse: {warehouse}\")\n    print(\"Start Date:\", X_processed[\"date\"].min())\n    print(\"End Date:\", X_processed[\"date\"].max())\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:55.928927Z","iopub.execute_input":"2024-08-06T17:16:55.929278Z","iopub.status.idle":"2024-08-06T17:16:55.949855Z","shell.execute_reply.started":"2024-08-06T17:16:55.929249Z","shell.execute_reply":"2024-08-06T17:16:55.94863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review dataset before training\nprint(len(processed_train_data_dict[warehouse][0]))\nprocessed_train_data_dict[warehouse][0].head()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:55.951332Z","iopub.execute_input":"2024-08-06T17:16:55.951671Z","iopub.status.idle":"2024-08-06T17:16:56.090819Z","shell.execute_reply.started":"2024-08-06T17:16:55.951641Z","shell.execute_reply":"2024-08-06T17:16:56.089611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model\n<br>We use XGBoost model with standard settings\n<br>Training loop - train each timeline separately.\n<br>Ready for ensemble modeling","metadata":{}},{"cell_type":"code","source":"#print nan values for every warehouse in train dataset dict:\nfor warehouse in processed_train_data_dict.keys():\n    print(f\"Nan X values for {warehouse}: {processed_train_data_dict[warehouse][0].isna().sum().sum()}\")\n    print(f\"nan y values for {warehouse}: {sum(np.isnan(processed_train_data_dict[warehouse][1]))}\")\n    \n    if FILL:\n        feat, targ = processed_train_data_dict[warehouse]\n\n        feat = feat.fillna(0)\n        targ = np.nan_to_num(targ, nan=0.0)\n        processed_train_data_dict[warehouse] = (feat, targ)\n\nif FILL:\n    print(\"\\n After FILL:\")\n    for warehouse in processed_train_data_dict.keys():\n        print(f\"Nan X values for {warehouse}: {processed_train_data_dict[warehouse][0].isna().sum().sum()}\")\n        print(f\"nan y values for {warehouse}: {sum(np.isnan(processed_train_data_dict[warehouse][1]))}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:56.092879Z","iopub.execute_input":"2024-08-06T17:16:56.093331Z","iopub.status.idle":"2024-08-06T17:16:56.132654Z","shell.execute_reply.started":"2024-08-06T17:16:56.093291Z","shell.execute_reply":"2024-08-06T17:16:56.131112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Loop\nTraining loop, see results of each model for each warehouse.","metadata":{}},{"cell_type":"code","source":"#### Training loop:\n\nval_data = {}\nmape_results = {}\ntraining_features = {}\nmape_df = {}\n\nfor warehouse, (X_train_scaled, y_train_scaled) in processed_train_data_dict.items():\n    print(f\"Training model for warehouse: {warehouse}\")\n\n    X_train_scaled = X_train_scaled.drop(columns=['id', 'warehouse', 'date'])\n    \n    training_features[warehouse] = X_train_scaled.columns.tolist()\n    \n    X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y_train_scaled, test_size=0.1, shuffle=True)\n    \n    val_data[warehouse] = (X_val, y_val)\n\n    mape_results[warehouse] = []\n\n    models = {\n        'XGBRegressor': XGBRegressor(),\n        'HistGradientBoostingRegressor': HistGradientBoostingRegressor(),\n        'LGBMRegressor': LGBMRegressor(),\n        'RandomForestRegressor': RandomForestRegressor(),\n        'BaggingRegressor': BaggingRegressor(),\n        'ExtraTreesRegressor': ExtraTreesRegressor(),\n        'GradientBoostingRegressor': GradientBoostingRegressor(),\n        'DecisionTreeRegressor': DecisionTreeRegressor(),\n        'ExtraTreeRegressor': ExtraTreeRegressor(),\n        'AdaBoostRegressor': AdaBoostRegressor(),\n        'KNeighborsRegressor': KNeighborsRegressor(),\n        \n        'CatBoostRegressor': CatBoostRegressor(silent=True),\n        'SVR': SVR(),\n        'ElasticNet': ElasticNet(),\n        'MLPRegressor': MLPRegressor(max_iter=1000)\n    }\n    \n    # Train, predict, and calculate MAPE for each model\n    for model_name, model in models.items():\n\n        model.fit(X_train, y_train)\n\n        # Predict on validation set\n        pred_val = model.predict(X_val)\n\n        mape = mean_absolute_percentage_error(y_val, pred_val)\n        mape_results[warehouse].append({'Model': model_name, 'MAPE': mape})\n\n    # Convert results to DataFrame for better readability\n    mape_df[warehouse] = pd.DataFrame(mape_results[warehouse])\n\n    # Format MAPE to 5 decimal places\n    mape_df[warehouse]['MAPE'] = mape_df[warehouse]['MAPE'].apply(lambda x: f\"{x:.5f}\")\n    mape_df[warehouse] = mape_df[warehouse].sort_values(by='MAPE')\n\n    print(\"MAPE Results for Each Model:\")\n    print(mape_df[warehouse])","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:16:56.134737Z","iopub.execute_input":"2024-08-06T17:16:56.135083Z","iopub.status.idle":"2024-08-06T17:17:47.029701Z","shell.execute_reply.started":"2024-08-06T17:16:56.135055Z","shell.execute_reply":"2024-08-06T17:17:47.028512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Training loop\n# Choosen XGBoost model, get score on validation dataset\n\nval_data = {}\nmape_results = {}\ntraining_features = {}\nmape_df = {}\n\nfor warehouse, (X_train_scaled, y_train_scaled) in processed_train_data_dict.items():\n    print(f\"Training model for warehouse: {warehouse}\")\n\n    X_train_scaled = X_train_scaled.drop(columns=['id', 'warehouse', 'date'])\n    \n    training_features[warehouse] = X_train_scaled.columns.tolist()\n    \n    X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y_train_scaled, test_size=0.1, shuffle=True)\n    \n    val_data[warehouse] = (X_val, y_val)\n    mape_results[warehouse] = []\n\n    model = XGBRegressor()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_val)\n    \n    mape = mean_absolute_percentage_error(y_val, y_pred)\n    \n    print(f\"\\nMAPE for XGBRegressor: {mape:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:47.031455Z","iopub.execute_input":"2024-08-06T17:17:47.032185Z","iopub.status.idle":"2024-08-06T17:17:49.113876Z","shell.execute_reply.started":"2024-08-06T17:17:47.032141Z","shell.execute_reply":"2024-08-06T17:17:49.112683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print feature importances\nfeature_importances = model.feature_importances_\nfeatures = X_train_scaled.columns if hasattr(X_train_scaled, 'columns') else np.arange(X_train_scaled.shape[1])\n\nprint(\"\\nFeature Importances:\")\nimportance_df = pd.DataFrame({\n    'Feature': features,\n    'Importance': feature_importances\n})\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\nprint(importance_df)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:49.115412Z","iopub.execute_input":"2024-08-06T17:17:49.1158Z","iopub.status.idle":"2024-08-06T17:17:49.128002Z","shell.execute_reply.started":"2024-08-06T17:17:49.115766Z","shell.execute_reply":"2024-08-06T17:17:49.126616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save all scaler features into a single json scaler config file and save the scaler config so it can be loded for inference.","metadata":{}},{"cell_type":"code","source":"X_train_scaled.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:49.129299Z","iopub.execute_input":"2024-08-06T17:17:49.129635Z","iopub.status.idle":"2024-08-06T17:17:49.267426Z","shell.execute_reply.started":"2024-08-06T17:17:49.129606Z","shell.execute_reply":"2024-08-06T17:17:49.26622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#### Training loop: XGBoost\n# Fit XGBoost on each ware\n# Save trained models for future iunference\n\n\nval_data = {}\nmape_results = {}\ntraining_features = {}\nmape_df = {}\nwarehouses_trained = {}\n\nfor warehouse, (X_train_scaled, y_train_scaled) in processed_train_data_dict.items():\n    print(f\"Training model for warehouse: {warehouse}\")\n\n    X_train_scaled = X_train_scaled.drop(columns=['id', 'warehouse', 'date'])\n    \n    training_features[warehouse] = X_train_scaled.columns.tolist()\n    \n    # train-test split\n    X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y_train_scaled, test_size=0.1, shuffle=True)\n    \n    val_data[warehouse] = (X_val, y_val)\n    mape_results[warehouse] = []\n\n    # Define hyperparameters\n    best_params = {\n        'n_estimators': 100,\n        'learning_rate': 0.1,\n        'max_depth': 5,\n        'random_state': 42\n    }\n\n    model = XGBRegressor(**best_params)\n    model.fit(X_train, y_train)\n\n\n    y_pred = model.predict(X_val)\n\n    mape = mean_absolute_percentage_error(y_val, y_pred)\n    print(f\"\\nMAPE: {mape:.5f}\")\n\n    # Save the trained model\n    #model.save_model(f'model_{warehouse}.json')\n    joblib.dump(model, f'model_{warehouse}.joblib')\n    \n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:49.274196Z","iopub.execute_input":"2024-08-06T17:17:49.274658Z","iopub.status.idle":"2024-08-06T17:17:51.144935Z","shell.execute_reply.started":"2024-08-06T17:17:49.274624Z","shell.execute_reply":"2024-08-06T17:17:51.143863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n#### Training loop: Ensemble\n\nval_data = {}\nmape_results = {}\ntraining_features = {}\nmape_df = {}\nwarehouses_trained = {}\n\nfor warehouse, (X_train_scaled, y_train_scaled) in processed_train_data_dict.items():\n    print(f\"Training model for warehouse: {warehouse}\")\n\n    X_train_scaled = X_train_scaled.drop(columns=['id', 'warehouse', 'date'])\n    \n    training_features[warehouse] = X_train_scaled.columns.tolist()\n    \n    X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y_train_scaled, test_size=0.1, shuffle=True)\n    \n    val_data[warehouse] = (X_val, y_val)\n\n    mape_results[warehouse] = []\n    \n    stacking_train = np.zeros((X_train.shape[0], 8))\n    stacking_test = np.zeros((X_test.shape[0],8))\n    \n    n_splits = 10\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n    \n    # Define the best hyperparameters\n    catboost_params = {\n        'iterations': 1000,\n        'depth': 10,\n        'learning_rate': 0.05,\n        'l2_leaf_reg': 3,\n        'bagging_temperature': 0.2,\n        'border_count': 254,\n        'verbose': 100\n    }\n\n    randomforest_params = {\n        'n_estimators': 200,\n        'max_depth': 9,\n        'min_samples_split': 5,\n        'min_samples_leaf': 2,\n        'max_features': 'auto',\n        'bootstrap': True,\n        'random_state': 42\n    }\n\n    gradientboosting_params = {\n        'n_estimators': 200,\n        'max_depth': 9,\n        'learning_rate': 0.1,\n        'min_samples_split': 5,\n        'min_samples_leaf': 2,\n        'max_features': 'auto'\n    }\n\n\n    models = [\n        ('xgb', XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=9, min_child_weight=5, subsample=0.9, colsample_bytree=0.9)),\n        ('lgbm', LGBMRegressor(n_estimators=200, learning_rate=0.1, max_depth=9)),\n        ('catboost', CatBoostRegressor(silent=True, n_estimators=200, depth=9, learning_rate=0.1)),\n        ('rf', RandomForestRegressor(n_estimators=200, max_depth=9, min_samples_split=5, min_samples_leaf=2)),\n        ('et', ExtraTreesRegressor(n_estimators=200, max_depth=9, min_samples_split=5, min_samples_leaf=2)),\n        ('gbr', GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=9)),\n        ('svr', SVR()),\n        ('elasticnet', ElasticNet()),\n        ('mlp', MLPRegressor(max_iter=1000))\n    ]\n    \n    \n    ensemble_model = VotingRegressor(estimators=models)\n    ensemble_model.fit(X_train, y_train)\n\n    \n    # Predict on validation set\n    y_pred = ensemble_model.predict(X_val)\n\n    mape = mean_absolute_percentage_error(y_val, y_pred)\n    print(f\"\\nMAPE for Optimized XGBRegressor: {mape:.5f}\")\n\n    # Save the trained model\n    #ensemble_model.save_model(f'model_{warehouse}.json')\n    joblib.dump(ensemble_model, f'model_{warehouse}.joblib')\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:51.146417Z","iopub.execute_input":"2024-08-06T17:17:51.146889Z","iopub.status.idle":"2024-08-06T17:17:51.158874Z","shell.execute_reply.started":"2024-08-06T17:17:51.146849Z","shell.execute_reply":"2024-08-06T17:17:51.157793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save configuration from training.\n# To be loaded on inference task - end to end approach\n\nscaler_config = {\n    'bin_features': binary_features,\n    'cat_features': categorical_features,\n    'scaler_features': x_scaler_features,\n    'training_features': training_features,\n}\n\njoblib.dump(scaler_config, 'scaler_config_mt.joblib')","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:51.160036Z","iopub.execute_input":"2024-08-06T17:17:51.160376Z","iopub.status.idle":"2024-08-06T17:17:51.183277Z","shell.execute_reply.started":"2024-08-06T17:17:51.16035Z","shell.execute_reply":"2024-08-06T17:17:51.182166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MAPE Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Inference and Visualisation on the TEST Data:","metadata":{}},{"cell_type":"markdown","source":"Here we deploy our trained models and run inference on the test data from the challenge. The test data are already preprocessed from the previous step, so we only load the test dataset csv, then predict. If we wanted to take raw test data, we would only need to utilize the \"process_data\" function from the above section first.","metadata":{}},{"cell_type":"markdown","source":"Function that that predicts the test data on multiple timelines - splitting the data for each warehouse, using multiple models.","metadata":{}},{"cell_type":"code","source":"def predict_multiple(data, warehouse_models_and_scalers, training_features):\n    predictions = []\n    submission_data = []\n\n    for warehouse in data['warehouse'].unique():\n        if warehouse not in warehouse_models_and_scalers:\n            print(f\"No model found for warehouse: {warehouse}\")\n            warehouse_predictions = [np.nan] * len(data[data['warehouse'] == warehouse])\n            predictions.extend(warehouse_predictions)\n            continue\n        \n        model_data = warehouse_models_and_scalers[warehouse]\n        model = model_data['model']\n        y_scaler = model_data['y_scaler']\n        wh_data = data[data['warehouse'] == warehouse].copy()\n        wh_data = wh_data.reindex(columns=training_features[warehouse])\n\n        preds = model.predict(wh_data)\n\n        preds_rescaled = y_scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n\n        # Align predictions with original data\n        wh_predictions = np.full(len(wh_data), np.nan)\n        \n        # Assign predictions to the corresponding indices in wh_predictions\n        wh_predictions[:len(preds_rescaled)] = preds_rescaled\n\n        predictions.extend(wh_predictions)\n\n        wh_submission_data = data.loc[data['warehouse'] == warehouse].copy()\n        wh_submission_data['predicted_orders'] = wh_predictions\n\n        submission_data.append(wh_submission_data)\n\n    # Check if submission_data is not empty before concatenation\n    if submission_data:\n        submission_df = pd.concat(submission_data)\n        submission_df.dropna(inplace=True)\n    else:\n        submission_df = pd.DataFrame()\n\n    return np.array(predictions), submission_df","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:51.18465Z","iopub.execute_input":"2024-08-06T17:17:51.185006Z","iopub.status.idle":"2024-08-06T17:17:51.198599Z","shell.execute_reply.started":"2024-08-06T17:17:51.184978Z","shell.execute_reply":"2024-08-06T17:17:51.196832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the saved models and scalers for each warehouse. This is optional and a conscious choise to use and is a part of end-to-end approach with potential deployment in mind.","metadata":{}},{"cell_type":"code","source":"def load_models_and_scalers(warehouses):\n    warehouse_models_and_scalers = {}\n\n    for wh in warehouses:\n        try:\n\n            #model.load_model(f'model_{wh}.json')\n            model = joblib.load(f'model_{wh}.joblib')\n                \n            x_scaler = joblib.load(f'x_scaler_{wh}.joblib')\n            y_scaler = joblib.load(f'y_scaler_{wh}.joblib')\n            encoder = joblib.load(f'onehot_scaler_{wh}.joblib')\n            \n            warehouse_models_and_scalers[wh] = {\n                'model': model,\n                'x_scaler': x_scaler,\n                'y_scaler': y_scaler,\n                'encoder': encoder\n            }\n        except Exception as e:\n            print(f\"Error loading model or scaler for warehouse {wh}: {str(e)}\")\n\n    return warehouse_models_and_scalers","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:51.200566Z","iopub.execute_input":"2024-08-06T17:17:51.201776Z","iopub.status.idle":"2024-08-06T17:17:51.218097Z","shell.execute_reply.started":"2024-08-06T17:17:51.201728Z","shell.execute_reply":"2024-08-06T17:17:51.216225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we load the scaler configuration and other metadata from the training stage","metadata":{}},{"cell_type":"code","source":"scaler_config = joblib.load('scaler_config_mt.joblib')  # load the scaler config\n\nx_scaler_features = scaler_config['scaler_features']  # standard scaler features in the right order that the training dataset was scaled on\ncategorical_features = scaler_config['cat_features']  # categorical features\nbinary_features = scaler_config['bin_features']  # binary features are unplugged before scaling and then plugged back in after scaling\ntraining_features = scaler_config['training_features']  # training features dictionary\n\nprint(f\"cat_features: {categorical_features}\")\nprint(f\"binary_features: {binary_features}\")\nprint(f\"scaler_columns: {len(x_scaler_features)}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:51.22002Z","iopub.execute_input":"2024-08-06T17:17:51.220411Z","iopub.status.idle":"2024-08-06T17:17:51.242997Z","shell.execute_reply.started":"2024-08-06T17:17:51.220381Z","shell.execute_reply":"2024-08-06T17:17:51.241518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Load the saved preprocessed training data","metadata":{}},{"cell_type":"code","source":"# Load test data\ndataframe_raw = pd.read_csv(\"/kaggle/working/test_proc_mt.csv\")\ndataframe_raw['date'] = pd.to_datetime(dataframe_raw['date'])\ndataframe = dataframe_raw.sort_values(by=['date', 'warehouse'])\nprint(f\"Rows: {len(dataframe)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:51.244665Z","iopub.execute_input":"2024-08-06T17:17:51.245126Z","iopub.status.idle":"2024-08-06T17:17:51.281064Z","shell.execute_reply.started":"2024-08-06T17:17:51.245086Z","shell.execute_reply":"2024-08-06T17:17:51.279616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:51.28254Z","iopub.execute_input":"2024-08-06T17:17:51.283216Z","iopub.status.idle":"2024-08-06T17:17:51.448566Z","shell.execute_reply.started":"2024-08-06T17:17:51.283169Z","shell.execute_reply":"2024-08-06T17:17:51.447302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load models and predict the test dataset","metadata":{}},{"cell_type":"code","source":"warehouse_lengths = test_df.groupby('warehouse').size()\n\nfor warehouse, length in warehouse_lengths.items():\n    print(f\"{warehouse}: {length}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:51.449875Z","iopub.execute_input":"2024-08-06T17:17:51.450234Z","iopub.status.idle":"2024-08-06T17:17:51.457181Z","shell.execute_reply.started":"2024-08-06T17:17:51.450205Z","shell.execute_reply":"2024-08-06T17:17:51.456144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"warehouse_models = load_models_and_scalers(warehouses)\npredictions, submission_df = predict_multiple(dataframe, warehouse_models, training_features)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:51.458689Z","iopub.execute_input":"2024-08-06T17:17:51.459146Z","iopub.status.idle":"2024-08-06T17:17:51.665332Z","shell.execute_reply.started":"2024-08-06T17:17:51.45904Z","shell.execute_reply":"2024-08-06T17:17:51.664175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:51.666727Z","iopub.execute_input":"2024-08-06T17:17:51.667064Z","iopub.status.idle":"2024-08-06T17:17:51.820772Z","shell.execute_reply.started":"2024-08-06T17:17:51.667036Z","shell.execute_reply":"2024-08-06T17:17:51.819495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = submission_df.rename(columns={\"predicted_orders\": \"orders\"})\n\nsubmission_df = submission_df[submission_df['date'].isin(test_df_raw['date'])]\n\nsubmission_export = submission_df[[\"id\", \"orders\"]]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:51.82237Z","iopub.execute_input":"2024-08-06T17:17:51.822735Z","iopub.status.idle":"2024-08-06T17:17:51.832656Z","shell.execute_reply.started":"2024-08-06T17:17:51.822688Z","shell.execute_reply":"2024-08-06T17:17:51.831578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(processed_test_df))\nprint(len(submission_df))\nprint(sum(submission_df.isna().sum()))","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:51.833913Z","iopub.execute_input":"2024-08-06T17:17:51.834239Z","iopub.status.idle":"2024-08-06T17:17:51.846944Z","shell.execute_reply.started":"2024-08-06T17:17:51.834212Z","shell.execute_reply":"2024-08-06T17:17:51.845609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:51.848445Z","iopub.execute_input":"2024-08-06T17:17:51.848894Z","iopub.status.idle":"2024-08-06T17:17:52.008772Z","shell.execute_reply.started":"2024-08-06T17:17:51.848853Z","shell.execute_reply":"2024-08-06T17:17:52.007497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_export.to_csv(\"/kaggle/working/submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:52.01241Z","iopub.execute_input":"2024-08-06T17:17:52.012788Z","iopub.status.idle":"2024-08-06T17:17:52.021639Z","shell.execute_reply.started":"2024-08-06T17:17:52.012759Z","shell.execute_reply":"2024-08-06T17:17:52.020502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualise the Test Data Predictions\nVisualise the predictions for 2 months of the test data with the actual data from previous year to see how the predictions fit.","metadata":{}},{"cell_type":"code","source":"# Plot predictions for each warehouse\ntrain_df_raw = pd.read_csv(\"/kaggle/input/rohlik-orders-forecasting-challenge/train.csv\")\ntrain_df_raw['date'] = pd.to_datetime(train_df_raw['date'])\n\nlast_year_data = train_df_raw[train_df_raw['date'].dt.year > 2022]\n\nfor wh in warehouses:\n\n    wh_df_pred = submission_df[submission_df['warehouse'] == wh]\n    wh_df_pred = wh_df_pred.sort_values(by=['date'])\n    wh_df_last_year = last_year_data[last_year_data['warehouse'] == wh]\n    wh_df_last_year = wh_df_last_year.sort_values(by=['date'])\n    \n    # Plot data (training data from 2023)\n\n    plt.figure(figsize=(12, 6))\n    \n    plt.plot(wh_df_pred.date, wh_df_pred['orders'], label='Predicted',  linestyle='--', color='red')\n\n    plt.plot(wh_df_last_year.date, wh_df_last_year['orders'], label='Actual', color='blue')\n\n    plt.title(f'Predicted vs Actual Orders for {wh} in 2023-2024')\n    plt.xlabel('Date')\n    plt.ylabel('Orders')\n    plt.legend()\n    plt.grid(True)\n    plt.xticks(rotation=45)\n    plt.savefig(f'prediction_plot_{wh}.png')\n    plt.show()\n    plt.close()\n\nprint(\"Predictions complete. Submission file and plots created.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-06T17:17:52.023103Z","iopub.execute_input":"2024-08-06T17:17:52.023523Z","iopub.status.idle":"2024-08-06T17:17:56.355056Z","shell.execute_reply.started":"2024-08-06T17:17:52.023485Z","shell.execute_reply":"2024-08-06T17:17:56.353802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}